# %% [markdown]
# In this notebook we will try to resolve Wine dataset problem with task description in 'Problem statement.pdf' file.
# Firstly, let's import all neede libraries.

# %%
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.naive_bayes import ComplementNB
from sklearn.neighbors import NearestNeighbors
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.inspection import permutation_importance
from sklearn.preprocessing import StandardScaler, LabelEncoder
from scipy.stats import shapiro, f_oneway, kruskal, mannwhitneyu
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay

# %% [markdown]
# # EDA

# %% [markdown]
# Load our data and see first examples.

# %%
data = pd.read_csv('WINE.csv')
data.head()

# %% [markdown]
# Let's see some info about our data.

# %%
print(data.info())
print(data.describe(include='all'))

# %% [markdown]
# Plot the target values distribution to know whether classes are balanced or not.

# %%
sns.countplot(x=data['Target']);

# %% [markdown]
# Plot box plots to see whether our features have outliers and which of them do.

# %%
data.plot(kind='box', subplots=True, layout=(9, 2), figsize=(10, 40));

# %% [markdown]
# Boxplots by Target classes:

# %%
data.boxplot(['B'], by='Target', figsize=(10, 8));
data.boxplot(['E'], by='Target', figsize=(10, 8));
data.boxplot(['H'], by='Target', figsize=(10, 8));
data.boxplot(['V'], by='Target', figsize=(10, 8));


# %% [markdown]
# As we can see from the barplot above, all of the features have outliers (apart from 'Index' obviously) but it is hard to estimate their amount. We can only roughly say that there are features that have not many outliers: 'Alcohol', 'Density' and 'U'.
# We can try out to remove outliers by 2nd and 98th percentiles.
# Also we hardly can see features distribution so it would be better to plot it in the next step.

# %%
# do not include 'Index' and 'Target' columns
for feature in data.columns[2:]:
    plt.figure()
    sns.histplot(data, x=feature, kde=True)

# %% [markdown]
# KDEs by Target classes:

# %%
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25, 7))
sns.kdeplot(data=data, x='V', hue='Target', palette='Set1', cbar_kws={'lw': 10}, shade=True, ax=ax1)
sns.kdeplot(data=data, x='E', hue='Target', palette='Set1', cbar_kws={'lw': 10}, shade=True, ax=ax2)
plt.show()
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25, 7))
sns.kdeplot(data=data, x='H', hue='Target', palette='Set1', cbar_kws={'lw': 10}, shade=True, ax=ax1)
sns.kdeplot(data=data, x='B', hue='Target', palette='Set1', cbar_kws={'lw': 10}, shade=True, ax=ax2)
plt.show()


# %% [markdown]
# We did not get much information from 'Density' feature distribution plot so we try a slightly different approcah: countplot.

# %%
s1 = sns.countplot(x=data[data['Type'] == 'white_wine']["Density"], color='yellow');
s2 = sns.countplot(x=data[data['Type'] == 'red_wine']["Density"], color='red');
plt.legend(labels=['White wine', 'Red wine']);

# %% [markdown]
# Do the train/validation split.

# %%
x = data.iloc[:, 2:]
y = data.iloc[:, 1]

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, shuffle=True, stratify=y)

data_train = x_train
data_train.insert(0, 'Target', y_train)
data_val = x_val
data_val.insert(0, 'Target', y_val)

# %% [markdown]
# ## Outliers handling: clamping and median
# 
# Get rid of the outliers. Define threshold as 2% so we took out data below 2 percentile and above 98 percentile.
# I don't touch 'Type' and 'Density' columns: first of them is categorical and second seems to have values entered erroneously (they are way bigger) and taking out 4% of values will drop all outliers, of course, but also valid values with them.
# For 'Density' it will be better to manually find anomalies and cut out just them.
# 
# Also it is a good question what to do with rows with outliers: delete them, replace with median or clamp using some min/max values.
# 
# I will not provide the code for the first approach because it has no sense, but just deleting rows with outliers cuts off almost 50% of the dataset which is unacceptable.
# So I try to replace outliers with median value and clamp them. Let's define the same 2nd and 98th percentiles as threshold for clamping.
# 
# I will use variable **'data_no_outliers_clamp'** for clamped dataset and **'data_no_outliers_median'** for data with outliers replaced with median.

# %%
# define low and hight percentiles
percentile_low = 0.02
percentile_high = 0.98

# we don't need 'Index' and 'Target' columns at all, as to 'Type' and 'Destiny' it is said above
data_no_outliers_clamp = data_train.drop(['Target', 'Type', 'Density'], axis=1)

# do clamping
for column in data_no_outliers_clamp:
    data_no_outliers_clamp[column] = data_no_outliers_clamp[column].clip(data_no_outliers_clamp[column].quantile(percentile_low), 
                                data_no_outliers_clamp[column].quantile(percentile_high), axis=0)
print(data_no_outliers_clamp.describe())

# return 'Type' and 'Target' columns
data_no_outliers_clamp.insert(1, 'Density', data_train.loc[:, 'Density'])
data_no_outliers_clamp.insert(0, 'Type', data_train.loc[:, 'Type'])
data_no_outliers_clamp.insert(0, 'Target', data_train.loc[:, 'Target'])

# replace all mistakenly added values of 'Density' with median and most popular class for the 'Type' column
data_no_outliers_clamp.loc[data_no_outliers_clamp['Density'] > 2, 'Density'] = data_no_outliers_clamp.loc[:, 'Density'].median()

# replace NaNs with median value of the column
if data_no_outliers_clamp.isnull().values.any():
    data_no_outliers_clamp.iloc[:, 2] = data_no_outliers_clamp.iloc[:, 2].fillna('white_wine')
    data_no_outliers_clamp.iloc[:, 3:] = data_no_outliers_clamp.iloc[:, 3:].fillna(data_no_outliers_clamp.iloc[:, 3:].median())

# check whether there are no NaNs left
print('Are there any NaNs?', data_no_outliers_clamp.isnull().values.any())

data_no_outliers_clamp.info()
data_no_outliers_clamp.describe()

# %% [markdown]
# Now I replace all outliers with median values of the corresponding columns.

# %%
# we don't need 'Index' and 'Target' columns at all, as to 'Type' and 'Destiny' it is said above
data_no_outliers = data_train.drop(['Target', 'Type', 'Density'], axis=1)

percentile_data = data_no_outliers.quantile([percentile_low, percentile_high])

# drop outliers by percentiles
data_no_outliers = data_no_outliers.apply(lambda x: x[(x > percentile_data.loc[percentile_low, x.name]) & 
                                                    (x < percentile_data.loc[percentile_high, x.name])], axis=0)

# return 'Type' and 'Target' columns
data_no_outliers.insert(1, 'Density', data_train.loc[:, 'Density'])
data_no_outliers.insert(0, 'Type', data_train.loc[:, 'Type'])
data_no_outliers.insert(0, 'Target', data_train.loc[:, 'Target'])                                                    

data_no_outliers_median = data_no_outliers.copy()

# there are NaNs where was outliers so replace them with median and most popular class for the 'Type' column
data_no_outliers_median.iloc[:, 1] = data_no_outliers_median.iloc[:, 1].fillna('white_wine')
data_no_outliers_median.iloc[:, 2:] = data_no_outliers_median.iloc[:, 2:].fillna(data_no_outliers_median.median())

# clean up 'Density' separately
data_no_outliers_median.loc[data_no_outliers_median['Density'] > 2, 'Density'] = data_no_outliers_median.loc[:, 'Density'].median()

# check whether there are no NaNs left
print('Are there any NaNs?', data_no_outliers_median.isnull().values.any())

data_no_outliers_median.info()
data_no_outliers_median.describe()

# %% [markdown]
# Let's see what we got after outliers replacing.
# 
# Below we can compare 'D' and 'Di' dependency on the raw data, after deleting outliers and after replacing them with median.

# %%
sns.set(font_scale=1)
r1 = sns.relplot(x=data_train['D'], y=data_train['U'], data=data_train, color='g')
r1.set(title='Raw Data')
r2 = sns.relplot(x=data_no_outliers['D'], y=data_no_outliers['U'], data=data_no_outliers)
r2.set(title='Data withour outliers')
r2 = sns.relplot(x=data_no_outliers_clamp['D'], y=data_no_outliers_clamp['U'], data=data_no_outliers_clamp, color='y')
r2.set(title='Outliers clamped')
r3 = sns.relplot(x=data_no_outliers_median['D'], y=data_no_outliers_median['U'], data=data_no_outliers_median, color='r')
r3.set(title='Outliers replaced with median');

# %% [markdown]
# Plot some other dependencies between a features

# %%
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
c1 = sns.scatterplot(data=data_no_outliers_clamp, x='D', y='Di', ax=ax1).set(title='Clamp')
m1 = sns.scatterplot(data=data_no_outliers_median, x='D', y='Di', ax=ax2).set(title='Median')

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
c2 = sns.scatterplot(data=data_no_outliers_clamp, x='Di', y='U', ax=ax1).set(title='Clamp')
m2 = sns.scatterplot(data=data_no_outliers_median, x='Di', y='U', ax=ax2).set(title='Median')

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
c3 = sns.scatterplot(data=data_no_outliers_clamp, x='A', y='Di', ax=ax1).set(title='Clamp')
m3 = sns.scatterplot(data=data_no_outliers_median, x='A', y='Di', ax=ax2).set(title='Median')

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
c4 = sns.scatterplot(data=data_no_outliers_clamp, x='D', y='A', ax=ax1).set(title='Clamp')
m4 = sns.scatterplot(data=data_no_outliers_median, x='D', y='A', ax=ax2).set(title='Median')

# %% [markdown]
# Below we see how changes 'A' feature distribution from raw data to no-outliers data, clamped and replaced with median data.

# %%
sns.histplot(data_train, x=data_train.A, kde=True).set(title='Raw Data');
plt.show()
sns.histplot(data_no_outliers, x=data_no_outliers.A, kde=True).set(title='No Outliers');
plt.show()
sns.histplot(data_no_outliers_clamp, x=data_no_outliers_clamp.A, kde=True).set(title='Outliers clamped');
plt.show()
sns.histplot(data_no_outliers_median, x=data_no_outliers_median.A, kde=True).set(title='Outliers replaced with Median');
plt.show()

# %% [markdown]
# Plot features distribution comparison for both approaches.

# %%
for feature in data_no_outliers_median.columns[1:]:
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
    sns.histplot(data_no_outliers_clamp, x=feature, ax=ax1, kde=True).set(title='Clamp')
    sns.histplot(data_no_outliers_median, x=feature, ax=ax2, kde=True).set(title='Median')
    plt.tight_layout()

# %% [markdown]
# ## Advantages of each approach
# 
# As we can find on many plots, some 'crosses' and 'frames' appeared there after clamping and "medianing" (most obvious is 'D' and 'Di' plot).
# It happened because we replaced outliers with median values and points were shifted to vertical and horizontal lines which lie on a median value.
# 'Frames' appeared because we replaced each value below and above lower and upper percentiles respectively with the relevant percentile and median 'cross' just has been pushed to the 'frame' which lies on the percentiles lines.
# 
# Watching on the distribution plot, distibution of the features that have been 'medianed' looks prettier but I concern that such a drastic change of outliers' value (from the extreme percentiles to the median) could have a negative effect on further model performance so for baseline models I will use clamped data.

# %% [markdown]
# ## Shapiro-Wilk Test
# 
# Do the Shapiro-Wilk test firstly for clamped, then 'medianed' data to see whether any feature has normal distribution.

# %%
print('Clamp: ')

for column in data_no_outliers_clamp.iloc[:, 2:]:
    stat, p = shapiro(data_no_outliers_clamp[column])

    print('Column {}: Statistics = {}, p = {}'.format(column, stat, p))

    if p > 0.05:
        print('Sample DOES look Normal')
    else:
        print('Sample does NOT look Normal')

# %%
print('Median: ')

for column in data_no_outliers_median.iloc[:, 2:]:
    stat, p = shapiro(data_no_outliers_median[column])

    print('Column {}: Statistics = {}, p = {}'.format(column, stat, p))

    if p > 0.05:
        print('Sample DOES look Normal')
    else:
        print('Sample does NOT look Normal')

# %% [markdown]
# As we see, none of our features is normally distributed.

# %% [markdown]
# Plot correlation matrices.

# %%
corr = data_no_outliers_median.corr()
f, ax = plt.subplots(figsize=(15, 15))
ax.set_title('Correlation Matrix, medianed data', fontsize=30)
sns.heatmap(corr, cmap='coolwarm', square=True, linewidths=1, cbar_kws={'shrink': .5});

# %%
corr = data_no_outliers_clamp.corr()
f, ax = plt.subplots(figsize=(15, 15))
ax.set_title('Correlation Matrix, clamped data', fontsize=30)
sns.heatmap(corr, cmap='coolwarm', square=True, linewidths=1, cbar_kws={'shrink': .5});

# %% [markdown]
# As we see from the matrices above, two features have the strongest positive correlation: 'D' and 'Di'.
# Also I would admit such pairs as 'D' and 'U'; 'Di' and 'U'.
# 
# Plot the dependency between 'D' and 'Di' features 

# %%
sns.set(font_scale=1)
sns.relplot(x=data_no_outliers_clamp['D'], y=data_no_outliers_clamp['Di'], data=data_no_outliers_clamp);

# %% [markdown]
# It is obvious that I can cut out one of them.
# To choose which one I compute both 'D' and 'Di' variances and leave the feature with the bigger one.

# %%
print('Variance of ''D'': ', data_no_outliers_median['D'].var())
print('Variance of ''Di'': ', data_no_outliers_median['Di'].var())


# %% [markdown]
# Based on this, I can drop 'D' feature.

# %% [markdown]
# ## ANOVA Test
# 
# This and two more above tests will show us whether all features originate from the same distribution or not. Thanks to them it is possible to decide which features are not important at all so I can drop them.

# %%
print('ANOVA for clamped data:\n')

for column in data_no_outliers_clamp.iloc[:, 2:]:

	stat, p = f_oneway(data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 1][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 2][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 3][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 4][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 5][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 6][column])

	print('Feature {}: Statistics = {}, p = {}'.format(column, stat, p))

	if p > 0.05:
		print('Looks like the same distribution')

print('\nANOVA for medianed data:\n')

for column in data_no_outliers_median.iloc[:, 2:]:

	stat1, p1 = f_oneway(data_no_outliers_median[data_no_outliers_median['Target'] == 1][column],
						data_no_outliers_median[data_no_outliers_median['Target'] == 2][column],
						data_no_outliers_median[data_no_outliers_median['Target'] == 3][column],
						data_no_outliers_median[data_no_outliers_median['Target'] == 4][column],
						data_no_outliers_median[data_no_outliers_median['Target'] == 5][column],
						data_no_outliers_median[data_no_outliers_median['Target'] == 6][column])

	print('Feature {}: Statistics = {}, p = {}'.format(column, stat1, p1))

	if p1 > 0.05:
		print('Looks like the same distribution')

# %% [markdown]
# ## Kruskal-Wallis Test

# %%
print('Kruskal-Wallis for clamped data:\n')

for column in data_no_outliers_clamp.iloc[:, 2:]:

	stat, p = kruskal(data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 1][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 2][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 3][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 4][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 5][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Target'] == 6][column])

	print('Feature {}: Statistics = {}, p = {}'.format(column, stat, p))

	if p > 0.05:
		print('Looks like the same distribution')

print('\nKruskal-Wallis for medianed data:\n')

for column in data_no_outliers_median.iloc[:, 2:]:

	stat1, p1 = kruskal(data_no_outliers_median[data_no_outliers_median['Target'] == 1][column],
						data_no_outliers_median[data_no_outliers_median['Target'] == 2][column],
						data_no_outliers_median[data_no_outliers_median['Target'] == 3][column],
						data_no_outliers_median[data_no_outliers_median['Target'] == 4][column],
						data_no_outliers_median[data_no_outliers_median['Target'] == 5][column],
						data_no_outliers_median[data_no_outliers_median['Target'] == 6][column])

	print('Feature {}: Statistics = {}, p = {}'.format(column, stat1, p1))

	if p1 > 0.05:
		print('Looks like the same distribution')

# %% [markdown]
# ## Mann-Whitney U Test

# %%
print('Mann-Whitney for clamped data:\n')

for column in data_no_outliers_clamp.iloc[:, 2:]:

	stat, p = mannwhitneyu(data_no_outliers_clamp[data_no_outliers_clamp['Type'] == 'white_wine'][column],
						data_no_outliers_clamp[data_no_outliers_clamp['Type'] == 'red_wine'][column])

	print('Feature {}: Statistics = {}, p = {}'.format(column, stat, p))

	if p > 0.05:
		print('Looks like the same distribution')

print('\nMann-Whitney for medianed data:\n')

for column in data_no_outliers_median.iloc[:, 2:]:

	stat1, p1 = mannwhitneyu(data_no_outliers_median[data_no_outliers_median['Type'] == 'white_wine'][column],
						data_no_outliers_median[data_no_outliers_median['Type'] == 'red_wine'][column])

	print('Feature {}: Statistics = {}, p = {}'.format(column, stat1, p1))

	if p1 > 0.05:
		print('Looks like the same distribution')

# %% [markdown]
# I can drop all features that have p>0.05 but if we look at the correlation matrix, we can find that ‘Alcohol’ feature has the highest correlation with the target feature so it is valuable for us.
# Finally, I will drop features: D, Nitrogen, V and E.

# %%
data_dropped = data_no_outliers_clamp.drop(['D', 'Nitrogen', 'V', 'E'], axis=1)
data_val = data_val.drop(['D', 'Nitrogen', 'V', 'E'], axis=1)

print(data_dropped.head())
print(data_val.head())

# %% [markdown]
# ## PCA

# %% [markdown]
# After dropping non-important features I will do the dimensionality reduction, PCA in first.

# %%
# don't include categorical 'Type' feature
X = data_dropped.iloc[:, 2:]

X = StandardScaler().fit_transform(X)

# %% [markdown]
# After fitting, I plot the cumulative variance graph using which I can decide how many principal components to leave.

# %%
pca = PCA().fit(X)
# see how many exactly variance we have from each component
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
print('Variance ratio descending: ', pca.explained_variance_ratio_)

# %% [markdown]
# Analyzing PCA results, we can see that we can use just 8 of principal components and lose only about 4% of variance. Let's do it.

# %%
pca = PCA(n_components=8)
principal_components = pca.fit_transform(X)

# %% [markdown]
# Couple of plots with Principal Components visualization by 'Target' and 'Type' features:

# %%
fig = plt.figure(figsize=(35, 5))
fig.suptitle('PCA by \'Target\' feature', fontsize=26)

for j in range(1, len(pca.explained_variance_ratio_)):
    ax = fig.add_subplot(1, 8, j)
    sns.scatterplot(x=principal_components[:, j], y=principal_components[:, 0],
                data=data_dropped['Target'],
                hue=data_dropped['Target'],
                alpha=0.5,
                palette=sns.color_palette('Set1', 6), 
                ax=ax)
    plt.xlabel('Principal component %i' %j)
    plt.ylabel('Pricipal component 0')

plt.tight_layout()
plt.show()

fig = plt.figure(figsize=(35, 5))
fig.suptitle('PCA by \'Type\' feature', fontsize=26)
for j in range(1, len(pca.explained_variance_ratio_)):
    ax = fig.add_subplot(1, 8, j+1)
    sns.scatterplot(x=principal_components[:, j], y=principal_components[:, 0],
                data=data_dropped['Type'],
                hue=data_dropped['Type'],
                alpha=0.5,
                palette=sns.color_palette('Set1', 2), 
                ax=ax)
    plt.xlabel('Principal component %i' %j)
    plt.ylabel('Pricipal component 0')
plt.tight_layout()
plt.show()

# %% [markdown]
# ## t-SNE

# %% [markdown]
# t-SNE is another technique for dimensionality reduction. I will reduce components number to 2 to better see the results on 2D visualization.

# %%
# don't use categorical feature in the 1st column
X_tsne = data_dropped.iloc[:, 2:]
X_tsne = StandardScaler().fit_transform(X_tsne)

X_emb = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X_tsne)

# %% [markdown]
# Plot t-SNE by 'Target' and 'Type' features.

# %%
sns.scatterplot(x=X_emb[:, 0], y=X_emb[:, 1], 
                hue=data_dropped['Target'], 
                legend='full', 
                palette=sns.color_palette('Set1', 6))
plt.title('t-SNE by \'Target\' feature', size=20)
plt.show();
plt.title('Seaborn heatmap - change font size')
sns.scatterplot(x=X_emb[:, 0], y=X_emb[:, 1], 
                hue=data_dropped['Type'], 
                legend='full', 
                palette=sns.color_palette('Set1', 2))
plt.title('t-SNE by \'Type\' feature', size=20)
plt.show();

# %% [markdown]
# As we can see, t-SNE did a pretty decent job on our data, if sort by 'Type' feature.

# %% [markdown]
# ## K-Means
# 
# As for the final part of EDA, I will apply K-Means algorithm to do blind clusterization on PCA-reduced data to 2 principal components, visualization and see what accuracy score we get.

# %%
# apply PCA before k-means to leave just 2 components
X_kmeans = data_dropped.iloc[:, 2:]
x_reduced = PCA(n_components=2).fit_transform(X_kmeans)

# apply k-means with 6 clusters
kmeans = KMeans(n_clusters=6)

# get predicted labels and centroids
y_kmeans = kmeans.fit_predict(x_reduced)
centroids = kmeans.cluster_centers_

# %%
# set a small step size of the mesh to increase visualization quality
h = 0.1

# plot decision boundaries
x_min, x_max = x_reduced[:, 0].min() - 1, x_reduced[:, 0].max() + 1
y_min, y_max = x_reduced[:, 1].min() - 1, x_reduced[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# get labels for each point in the mesh
z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])

# convert result to the color plot
z = z.reshape(xx.shape)
plt.figure(1, figsize=(25,10))
plt.clf()
# plot color areas for each cluster
plt.imshow(
    z,
    interpolation='nearest',
    extent=(xx.min(), xx.max(), yy.min(), yy.max()),
    cmap=plt.cm.Paired,
    aspect='auto',
    origin='lower'
)

# plot labels by 'Target' feature
plt.scatter(x_reduced[:, 0], x_reduced[:, 1], 
            c=y_kmeans, 
            cmap='plasma')

# plot centroids 
plt.scatter(
    centroids[:, 0],
    centroids[:, 1],
    marker='x',
    s=150,
    linewidths=3,
    color='black',
    zorder=10
)

plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.title('K-Means', size=20)
plt.xticks(())
plt.yticks(())
plt.show()

# %% [markdown]
# Now let's see in details how effective K-Means was: plot Confusion Matrix and Classification report.

# %%
# get confusion matrix and classification report using standard sklearn library
# I add 1 to y_kmeans because it starts numeration with 0 and our classes start with 1
cm = confusion_matrix(data_dropped['Target'], y_kmeans+1)
cr = classification_report(data_dropped['Target'], y_kmeans+1, output_dict=True)

# plot them
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(17, 8))
sns.heatmap(cm.T, annot=True, cmap='Blues', cbar=False, fmt='g', xticklabels=np.arange(1, 7), yticklabels=np.arange(1, 7), ax=ax1)
ax1.set_title('Confusion Matrix', size=25)
ax1.set_xlabel('True values')
ax1.set_ylabel('Predicted values')

sns.heatmap(pd.DataFrame(cr).iloc[:-1, :].T, annot=True, cmap='Blues', cbar=False, ax=ax2)
ax2.set_title('Classification report', size=25)
plt.show()

# %% [markdown]
# As we see from tables, we did not get much accuracy from K-Means with just 15%. But we could expect it after PCA plots with 'Target' variable: data points were distributed almost randomly with no evident clusters.

# %% [markdown]
# 
# ## DBSCAN

# %% [markdown]
# I also try the more complicated and effective clustering algorithm DBSCAN.  
# 
# It has 2 main parameters that we can tune: *epsilon* (maximum distance between 2 points to be considered as neighbors) and *min_samples* (number of samples in a neighborhood for a point to be considered as a core point).  
# 
# To find optimal epsilon parameter value for DBSCAN I should plot the K-distance Graph. For the K-distance Graph, in turn, I should obtain the distances between each point in dataset and the nearest point to them. I can get them using NearestNeighbors from sklearn.neighbors.

# %%
X_dbscan = data_dropped.iloc[:, 2:]
X_dbscan = StandardScaler().fit_transform(X_dbscan)
neigh = NearestNeighbors(n_neighbors=2)
nbrs = neigh.fit(X_dbscan)
distances, indices = nbrs.kneighbors(X_dbscan)

# %%
# Plotting K-distance Graph
distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.figure(figsize=(20,10))
plt.plot(distances)
plt.title('K-distance Graph',fontsize=20)
plt.xlabel('Data Points sorted by distance',fontsize=14)
plt.ylabel('Epsilon',fontsize=14)
plt.show()

# %% [markdown]
# Then, I should choose the according value on Y scale where graph has the maximum curvature. As seen on the plot, optimal epsilon would be equal to about 1.9.  
# I decided to leave 'min_samples' with default value, 5. 

# %%
clustering = DBSCAN(eps=1.9, min_samples=5).fit(X)
labels = clustering.labels_

print(np.unique(labels))

# %%
# get confusion matrix and classification report using standard sklearn library
# add 2 to labels number since they start from -1 and we need 1-6
cm = confusion_matrix(data_dropped['Target'], labels+2)
cr = classification_report(data_dropped['Target'], labels+2, output_dict=True)

# plot them
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(17, 8))
sns.heatmap(cm.T, annot=True, cmap='Blues', cbar=False, fmt='g', xticklabels=np.arange(1, 7), yticklabels=np.arange(1, 7), ax=ax1)
ax1.set_title('Confusion Matrix', size=25)
ax1.set_xlabel('True values')
ax1.set_ylabel('Predicted values')

sns.heatmap(pd.DataFrame(cr).iloc[:-1, :].T, annot=True, cmap='Blues', cbar=False, ax=ax2)
ax2.set_title('Classification report', size=25)
plt.show()

# %% [markdown]
# # Modeling
# 
# I need a baseline to compare model efficiency so I will apply some basic models without tuning any hyperparameters:
# - logistic regression
# - gradient boosting
# - naive bayes
# - decision tree
# - random forest
# 
# But firstly I should get rid of categorical data and replace it with dummy variable.

# %%
# handle dummy features
data_dropped2 = pd.get_dummies(data=data_dropped, drop_first=True)
data_val2 = pd.get_dummies(data=data_val, drop_first=True)

# handle NaNs: replace with median for numerical features and with most popular class for the 'Type' column
data_val2.iloc[:, 2:] = data_val2.iloc[:, 2:].fillna(data_val2.median())
data_val2.iloc[:, 1] = data_val2.iloc[:, 1].fillna('white_wine')
print('Are there any NaNs left?', data_val2.isnull().values.any())

# %%
x_train = data_dropped2.iloc[:, 1:]
y_train = data_dropped2.iloc[:, 0]
x_val = data_val2.iloc[:, 1:]
y_val = data_val2.iloc[:, 0]

# %% [markdown]
# Do the label encoding since the Target column is numerical and categorical is needed for the classification models.

# %%
le = LabelEncoder()

y_train_enc = le.fit_transform(y_train)
y_val_enc = le.fit_transform(y_val)

# %%
# prepare dictionary for models' scores
scores = {}

# %% [markdown]
# ## Logistic regression

# %% [markdown]
# For first, I will use the simplest Linear Regression to have some base to compare model scores.
# 
# Firstly, I should do the standardization because it will affect on Logistic regression converging time drastically.

# %%
x_train_scaled = StandardScaler().fit_transform(x_train)
x_val_scaled = StandardScaler().fit_transform(x_val)

# %%
# set such max_iter to have model converging
clf = LogisticRegression(max_iter=200)

clf.fit(x_train_scaled, y_train_enc)

# %%
y_pred_lr = clf.predict(x_val_scaled)

scores['Logistic'] = accuracy_score(y_val_enc, y_pred_lr)

print('Accuracy for logistic regression is', scores['Logistic'])

# %% [markdown]
# ## Boosting

# %%
boost = GradientBoostingClassifier().fit(x_train, y_train_enc)

# %%
y_pred_boost = boost.predict(x_val)

scores['Boosting'] = boost.score(x_val, y_val_enc)

print('Accuracy for boosting is', scores['Boosting'])

# %% [markdown]
# ## Naїve Bayes

# %%
bayes = ComplementNB().fit(x_train, y_train_enc)

# %%
y_pred_bayes = bayes.predict(x_val)

scores['Bayes'] = bayes.score(x_val, y_val_enc)

print('Accuracy for Bayes is', scores['Bayes'])

# %% [markdown]
# ## Decision Tree

# %%
dt = DecisionTreeClassifier().fit(x_train, y_train_enc)

# %%
y_pred_dt = dt.predict(x_val)

scores['Decision_tree'] = dt.score(x_val, y_val_enc)

print('Accuracy for Decision Tree:', scores['Decision_tree'])

# %% [markdown]
# ## Random Forest

# %%
rf = RandomForestClassifier()

rf.fit(x_train, y_train_enc)

# %%
y_pred_rf = rf.predict(x_val)

scores['Random_forest'] = rf.score(x_val, y_val_enc)

print('Accuracy for Random Forest:', scores['Random_forest'])

# %% [markdown]
# Raw untuned models performance:

# %%
plt.figure(figsize=(10, 5))
plt.title('Model performance', fontdict={'fontsize': 20})
plt.bar([key for key in scores], scores.values())
plt.show()

# %% [markdown]
# ## Confusion Matrices
# 
# Below I plot confusion matrices with prediction of all 5 algorithms.
# I can use them to analyse which classes are hard to predict for models and which are not.

# %%
# get all confusion matrices
cm_lr = confusion_matrix(y_val_enc, y_pred_lr, labels=clf.classes_)
cm_boost = confusion_matrix(y_val_enc, y_pred_boost, labels=boost.classes_)
cm_bayes = confusion_matrix(y_val_enc, y_pred_bayes, labels=bayes.classes_)
cm_dt = confusion_matrix(y_val_enc, y_pred_dt, labels=dt.classes_)
cm_rf = confusion_matrix(y_val_enc, y_pred_rf, labels=rf.classes_)

c_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=clf.classes_+1)
c_boost = ConfusionMatrixDisplay(confusion_matrix=cm_boost, display_labels=boost.classes_+1)
c_bayes = ConfusionMatrixDisplay(confusion_matrix=cm_bayes, display_labels=bayes.classes_+1)
c_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=dt.classes_+1)
c_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=rf.classes_+1)

# plot them
plt.rcParams['axes.grid'] = False

_, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(40, 8))
plt.rcParams.update({'font.size': 16})

c_lr.plot(cmap='Blues', colorbar=False, ax=ax1)
ax1.set_title('Confusion Matrix, Logistic Regression', size=25)
ax1.tick_params(labelsize=20)

c_boost.plot(cmap='Blues', colorbar=False, ax=ax2)
ax2.set_title('Confusion Matrix, Boosting', size=25)
ax2.tick_params(labelsize=20)

c_bayes.plot(cmap='Blues', colorbar=False, ax=ax3)
ax3.set_title('Confusion Matrix, Bayes', size=25)
ax3.tick_params(labelsize=20)

c_dt.plot(cmap='Blues', colorbar=False, ax=ax4)
ax4.set_title('Confusion Matrix, Decision Tree', size=25)
ax4.tick_params(labelsize=20)

c_rf.plot(cmap='Blues', colorbar=False, ax=ax5)
ax5.set_title('Confusion Matrix, Random Forest', size=25)
ax5.tick_params(labelsize=20)
plt.show()

# %% [markdown]
# ## Cross-Validation
# 
# Do the cross-validation for all 5 algorithms to evaluate all mean accuracies and standard deviations for each of them.

# %%
scores_lr = cross_val_score(clf, x_train_scaled, y_train_enc)
scores_boost = cross_val_score(boost, x_train, y_train_enc)
scores_bayes = cross_val_score(bayes, x_train, y_train_enc)
scores_tree = cross_val_score(dt, x_train, y_train_enc)
scores_rf = cross_val_score(rf, x_train, y_train_enc)

print('Scores for cross-val Logistic regression: {}\n {} mean accuracy with a standard deviation of {}\n'.format(scores_lr, scores_lr.mean(), scores_lr.std()))
print('Scores for cross-val Boosting: {}\n {} mean accuracy with a standard deviation of {}\n'.format(scores_boost, scores_boost.mean(), scores_boost.std()))
print('Scores for cross-val Naive Bayes: {}\n {} mean accuracy with a standard deviation of {}\n'.format(scores_bayes, scores_bayes.mean(), scores_bayes.std()))
print('Scores for cross-val Decision Tree: {}\n {} mean accuracy with a standard deviation of {}\n'.format(scores_tree, scores_tree.mean(), scores_tree.std()))
print('Scores for cross-val Random forest: {}\n {} mean accuracy with a standard deviation of {}\n'.format(scores_rf, scores_rf.mean(), scores_rf.std()))


# %% [markdown]
# ## Feature Importances

# %% [markdown]
# Get feature importances from 3 different sources: output attributes of the Logistic Regression and Decision Tree and Sklearn method ‘permutation_importance’ with the Random Forest model as estimator.

# %%
# sklearn built-in method using random forest classifier
perm_importance = permutation_importance(rf, x_val, y_val_enc)
# importance from logistic regression
importance_logreg = clf.coef_[0]
# importance from decision tree
importance_dt = dt.feature_importances_
# get column names
feature_names = data_dropped.columns[1:]

# %% [markdown]
# Plot the results.

# %%
sorted_idx = (-importance_logreg).argsort()
plt.figure(figsize=(10, 5))
plt.title('Feature Importance Logistic regression', fontdict={'fontsize': 20})
plt.bar(feature_names[sorted_idx], importance_logreg[sorted_idx])
plt.show()

sorted_idx = (-importance_dt).argsort()
plt.figure(figsize=(10, 5))
plt.title('Feature Importance Decision Tree', fontdict={'fontsize': 20})
plt.bar(feature_names[sorted_idx], importance_dt[sorted_idx])
plt.show()

sorted_idx = (-perm_importance.importances_mean).argsort()
plt.figure(figsize=(10, 5))
plt.title('Feature Importance sklearn (Random Forest)', fontdict={'fontsize': 20})
plt.bar(feature_names[sorted_idx], perm_importance.importances_mean[sorted_idx])
plt.show()

# %% [markdown]
# Do data logarithmization and Shapiro-Wilk test one more time hoping to get normal distribution for more features.

# %%
# don't include the last column since it has bool values and will give us infinity after lognorm
x_train_log = np.log(x_train.iloc[:, :-1])
# return categorical column
x_train_log.insert(len(x_train_log.columns), 'Type_white_wine', x_train.loc[:, 'Type_white_wine'])

# %% [markdown]
# Shapiro-Wilk test for lognorm data:

# %%
for column in x_train_log:
    stat, p = shapiro(x_train_log[column])
    if p > 0.05:
        print('Column {}: Statistics = {}, p = {}'.format(column, stat, p))
        print('Sample does look Normal')

# %% [markdown]
# As we can see, log normalization of data did not give us mych results so there is no need to do Naive Bayes one more time.


